{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Classification\n",
    "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we'll use the MNIST dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels, you can see a sample below\n",
    "\n",
    "\n",
    "<img src='https://user-images.githubusercontent.com/24764528/50548844-898ff580-0c79-11e9-8bad-9c7ad0a9abf1.png' width='400px'>\n",
    "Our goal is to build a neural network that can take one of these images and predict the digit in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets , transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "#Batch size determines we are getting 64 images at once \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have the training data loaded into trainloader and we make that an iterator with iter(trainloader). Later, we'll use this to loop through the dataset for training, like\n",
    "\n",
    "for image, label in trainloader:\n",
    "    ## do things with images and labels\n",
    "    \n",
    "You'll notice I created the trainloader with a batch size of 64, and shuffle=True.\n",
    "The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a batch.\n",
    "And shuffle=True tells it to shuffle the dataset every time we start going through the data loader again. But here I'm just grabbing the first batch so we can check out the data.\n",
    "\n",
    "We can see below that images is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Neural Net\n",
    "\n",
    " In fully-connected networks, the input to each layer must be a one-dimensional vector (which can be stacked into a 2D tensor as a batch of multiple examples).\n",
    "#### Flattening\n",
    "- link{https://www.youtube.com/watch?v=mFAIBMbACMA}\n",
    "\n",
    "However, our images are 28x28 2D tensors, so we need to convert them into 1D vectors. Thinking about sizes, we need to convert the batch of images with shape (64, 1, 28, 28) to a have a shape of (64, 784), 784 is 28 times 28. This is typically called flattening, we flattened the 2D images into 1D vectors.\n",
    "\n",
    "\n",
    "we need 10 output units for the 10 classes (digits). \n",
    "\n",
    "\n",
    "### Exercise:\n",
    "Flatten the batch of images. Then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  8.6133,  11.7412,  -6.5254, -12.6381,  -6.3289, -15.2273,  -0.5524,\n",
      "          25.2682, -17.5737,  -4.7067],\n",
      "        [  3.6202,  19.2320,   2.1570, -14.6308,  -7.5981, -11.4262,  -8.2019,\n",
      "          22.3428, -20.9903,  -1.1556],\n",
      "        [ 14.2610,  11.5073,  -7.5085,   4.2380, -18.0474, -22.7773,  -4.4314,\n",
      "          22.5972,   0.9979,   0.5807],\n",
      "        [  6.9035,   8.0865,  -7.7323, -15.3868, -12.6504, -20.6788,  -1.9003,\n",
      "          31.5397,  -6.6245,  -4.8954],\n",
      "        [  2.5797,  13.4820,  -5.6412, -10.7501,  -6.1787, -15.5740,  -2.6697,\n",
      "          15.7563, -17.4874,  -4.9271],\n",
      "        [  3.5354,  13.6537,  -6.1801, -10.5023, -10.3259, -20.0178,   4.5771,\n",
      "          19.8936, -11.7105,  -6.9272],\n",
      "        [  9.6405,  10.9627,  -4.4450, -13.5974, -12.8402, -22.7672,   0.6438,\n",
      "          22.1266, -13.0678,  -6.2751],\n",
      "        [ 12.6427,  12.3807, -11.1555,  -4.7580, -10.3622, -15.3402,  -2.1698,\n",
      "          18.0319,  -9.2950, -11.8716],\n",
      "        [ 12.1711,  10.2802,  -0.2230, -12.2097,  -5.0701, -13.4902,   2.4853,\n",
      "          19.7669, -13.3253, -10.8022],\n",
      "        [  1.8640,  10.9754,  -2.4413, -11.6363,  -9.2032, -18.7891,   7.1429,\n",
      "          16.0867,  -9.2053,   2.2292],\n",
      "        [ 17.0747,   6.8820,  -5.5983, -16.7475,  -3.8594, -17.1327,  -2.0051,\n",
      "          25.3900, -15.2003,  -6.3692],\n",
      "        [  6.4770,  15.0738,  -8.7115, -17.4609,  -9.0949, -11.7653,   3.7256,\n",
      "          25.1927, -14.4363,  -4.9056],\n",
      "        [  2.0818,  16.0493,   0.1694, -23.8890,  -3.3166, -11.4347,   1.2284,\n",
      "          22.4966,  -9.2260,  -5.1239],\n",
      "        [ 13.9248,  10.5222,   1.4966,  -8.4650,  -4.1826, -25.8600,   3.6992,\n",
      "          20.5330,   5.9005,   8.5602],\n",
      "        [ 16.4702,  10.5900,  -2.7751, -16.8020,  -9.4554,  -6.3769,   5.0879,\n",
      "          28.1837, -13.5012,  -6.8231],\n",
      "        [ 12.2287,  22.0967,   7.3365, -13.5732,  -0.6653, -16.6056,  -1.0484,\n",
      "          21.0877, -20.5662,   2.6974],\n",
      "        [  9.1643,   0.5597,  -3.7289, -16.4658, -11.6446, -21.3564,   0.9325,\n",
      "          19.4284, -11.2180,  -2.9385],\n",
      "        [ 13.6118,   8.5236,  -0.6247,  -3.7711, -13.3827, -18.0865,   1.2810,\n",
      "          27.5137,  -1.1001,  -4.4729],\n",
      "        [  6.0049,  10.6374,  -7.8042,  -8.6037,  -9.8390, -13.4238,  -0.4371,\n",
      "          25.8138, -10.4549,  -7.0713],\n",
      "        [  6.0228,  12.5350,  -3.0828, -17.2514,  -9.4368, -13.6602,   5.5031,\n",
      "          24.6349, -11.7259,   4.9651],\n",
      "        [  7.7017,  14.9548,  -7.9590, -14.6020,  -7.1425, -18.1792,   2.6591,\n",
      "          30.4340,  -6.3789,  -8.7699],\n",
      "        [  6.8769,  21.4839,  -1.9314, -22.1777,  -5.3152, -23.6593,   4.6590,\n",
      "          10.6721, -15.7973,   3.2952],\n",
      "        [  6.8051,  19.1055,   8.0896,  -7.1019,   2.0802, -11.6546,   1.0402,\n",
      "          16.6254,  -5.6033, -10.3265],\n",
      "        [  9.6589,   7.8947,  -6.1664, -10.6802,  -3.4140, -15.4864,   2.4637,\n",
      "          24.0487,  -6.0044,  -9.2072],\n",
      "        [  3.5447,  18.0597,  -5.1432, -12.8247,  -6.9823, -15.3821,  11.7714,\n",
      "          20.6679,  -4.0982,   1.4135],\n",
      "        [ 10.1627,   3.9767,  -2.7008, -23.6128, -16.9353, -14.2952,  -2.1276,\n",
      "          23.3415, -10.7870,   1.9690],\n",
      "        [ 11.9042,  23.6577,   0.3109, -16.6584,  -2.4784, -11.8606,  -0.1037,\n",
      "          22.1606, -17.5290,  -5.2245],\n",
      "        [ -3.9488,  10.3543,   3.4481, -17.8215,  -8.8228, -13.2065,   7.1405,\n",
      "          21.3960, -11.8224,  -2.1798],\n",
      "        [ 14.3086,  13.1051,   0.4934, -10.5341,  -1.8571, -30.3312,   0.1818,\n",
      "          22.2464, -11.1608,  -1.1909],\n",
      "        [ 11.3853,  13.5467,   0.0771,  -9.6301,  -5.9554, -15.5634,  -5.9016,\n",
      "          33.4399, -10.7957, -10.0323],\n",
      "        [  9.8827,  17.0734,   8.9245, -12.0013,  -2.7112,  -8.8634,   3.8694,\n",
      "          28.4393, -10.6605,  -8.2422],\n",
      "        [  4.7967,   5.5451,  -1.6221, -22.6794,  -5.7410, -20.8990,  -1.7460,\n",
      "          20.4018,  -3.7683,  -5.8767],\n",
      "        [ 10.4379,  10.4445,  -1.4542, -17.8892,   1.4725, -24.1525,  -1.5574,\n",
      "          35.4833,  -8.6693,  -8.6253],\n",
      "        [  6.0488,  19.6339,  -5.9645, -24.8068,  -5.5574, -18.4859,   2.0937,\n",
      "          25.8633,  -8.4859,  -8.4107],\n",
      "        [ 10.7934,  13.2703,  -3.8008, -14.8265, -13.3517, -18.7385,   4.4883,\n",
      "          31.8327, -11.8633,   5.3002],\n",
      "        [  6.9976,  14.8550, -10.4292, -20.9611, -13.2766, -12.2199,   0.6576,\n",
      "          24.2523,  -5.0847,  -4.7863],\n",
      "        [ 11.2098,   9.8329,  -6.2070, -16.8633,   2.5396, -18.6667,  13.0735,\n",
      "          20.3989, -19.1022,  -1.7139],\n",
      "        [ 10.4408,  16.3630,   5.1532, -16.0873,  -5.4792, -14.9969,  -2.6836,\n",
      "          23.1951,  -3.2440,  -1.6005],\n",
      "        [  4.2533,   2.0533,  -2.8470, -24.1191,  -2.4770, -16.3592,  -0.9150,\n",
      "          33.0106,  -7.8249,  -5.8583],\n",
      "        [  4.3577,  18.5258,  -2.3874, -15.1106,  -8.7277, -20.0052,   3.7710,\n",
      "          25.5865,   4.7830,  -6.2765],\n",
      "        [  4.2316,  20.9679,   2.5710, -17.3969,  -1.4898, -16.7093,   3.9783,\n",
      "          18.8924, -11.5679,  -6.7755],\n",
      "        [  3.0081,  20.0081,   7.7491, -13.6710,   2.3119, -17.0451,   0.1037,\n",
      "          30.6408,  -4.4350,  -1.5406],\n",
      "        [ -1.9014,   9.9558,  -3.4303, -12.6123, -15.9747, -15.5700,   2.5929,\n",
      "          22.4854,  -2.1074,  -6.4492],\n",
      "        [ 17.6639,   9.7567,  -5.0853, -13.7159,  -8.0004, -24.0400,   0.7372,\n",
      "          17.1828, -17.0834,  -1.4130],\n",
      "        [ 11.0213,  15.5233,  -4.6745, -23.0256, -12.0089, -15.3821,   0.9090,\n",
      "          25.7428, -15.1599,  -2.0394],\n",
      "        [  9.9959,   9.5224,  -2.8539, -25.2385,  -7.7497, -19.6890,   4.9588,\n",
      "          32.7117, -12.3857,  -8.7865],\n",
      "        [ 12.0001,  20.1584,   1.2349, -12.8424,   2.6463, -14.9321,   4.1452,\n",
      "          19.7760, -21.1151,  -5.0830],\n",
      "        [ 14.2862,  10.2669,  -4.1272,  -6.3332,  -5.9497, -17.3524,   4.5487,\n",
      "          22.4769, -21.5637,  -7.3758],\n",
      "        [  6.4514,   5.3910,  -5.9531, -25.0559,   4.9506, -13.3187,  -3.4632,\n",
      "          24.3576,  -1.1964, -12.2317],\n",
      "        [  8.8438,  24.0101,  -4.3708, -15.4483, -11.2882, -21.0067,   2.6728,\n",
      "          18.0978,  -9.7058,  -4.8827],\n",
      "        [  5.2652,   9.1377, -10.5634,  -9.0884,  -9.5552, -19.8348,   0.3670,\n",
      "          25.8516,  -6.5429,  -7.4122],\n",
      "        [  0.4644,  16.9111,  -5.7947, -16.3387,  -2.4894, -11.4397,  -2.5025,\n",
      "          16.2540, -14.3073,  -1.1334],\n",
      "        [  0.4332,  13.7763,  -0.5005, -12.5534,  -5.7595, -12.1632,   2.0332,\n",
      "          17.2518,  -0.6724, -11.5638],\n",
      "        [  7.4438,   7.4665,  -4.6204, -17.2070,  -9.0546, -10.5752,   1.3304,\n",
      "          21.1809,  -9.6205,  -2.2533],\n",
      "        [ 15.1247,   7.3221,  -6.9841, -15.6696,  -6.4988, -10.4955,   0.0912,\n",
      "          22.6360,  -2.7883,  -3.1642],\n",
      "        [  7.3902,   9.3195,  -5.1292, -21.6306,  -4.6346, -14.6370,   2.5535,\n",
      "          16.8206, -14.4732,  -0.7679],\n",
      "        [  7.3282,  10.2722,  -4.2867, -10.9815,  -2.8802, -16.2902,   0.1874,\n",
      "          17.6586, -17.1987,  -3.2044],\n",
      "        [ -0.7762,  12.9301,  -3.5360, -12.5320, -11.1303, -14.1659,   1.4495,\n",
      "          26.6717,  -8.4404,  -4.1702],\n",
      "        [ 12.4790,   9.5135,  -5.1546, -23.9727, -13.9786, -17.4828,   1.9771,\n",
      "          21.0333,  -0.5286,  -1.0401],\n",
      "        [  1.8215,  13.0959,  -1.3973, -14.6936,  -9.7804, -18.2441,   1.6816,\n",
      "           8.8839,  -9.5060,   1.6853],\n",
      "        [  2.5403,  11.7267, -15.2754, -11.8508,  -9.8284, -17.1309,   9.4035,\n",
      "          30.3637,  -6.9264,  -2.0288],\n",
      "        [ 10.2751,  15.5213,   1.2700, -23.9907,  -8.0047, -12.8702,   1.0389,\n",
      "          26.0477,  -7.1082,  -5.8767],\n",
      "        [ 15.2582,  10.5976,  -5.7822, -14.5802, -13.5158, -13.6364,  -1.1065,\n",
      "          30.7293,  -6.9822,  -4.7893],\n",
      "        [ 11.3238,  11.6382,  -6.8903, -19.3284,  -5.4625, -25.4167,   3.1223,\n",
      "          29.5378, -13.9373,  -5.1942]])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "#Reshaping the images \n",
    "# inputs = images.view(images.shape[0],-1)\n",
    "# images.shape[0] return batch size , putting -1 is a quick way to flatten a tensor without knowing second dimension\n",
    "# so final size 64,784\n",
    "inputs = images.view(64,784)\n",
    "n_hidden = 256\n",
    "n_output = 10\n",
    "\n",
    "# w1 and B1 weights and bias for hidden layer\n",
    "w1 = torch.randn((784,256))\n",
    "w2 = torch.randn((256,10))\n",
    "B1 = torch.randn((1,256))\n",
    "B2 = torch.randn((1,10))\n",
    "\n",
    "def activation(x):\n",
    "    return 1/(1  + torch.exp(-x))\n",
    "\n",
    "h = activation(torch.mm(inputs,w1) + B1)\n",
    "output = torch.mm(h,w2) + B2\n",
    "\n",
    "print(output)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we have 10 outputs for our network. We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to.\n",
    "\n",
    "To calculate this probability distribution, we often use the softmax function'. Mathematically this looks like\n",
    "\n",
    "$$\n",
    "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
    "$$\n",
    "What this does is squish each input $x_i$ between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one.\n",
    "\n",
    "Exercise: Implement a function softmax that performs the softmax calculation and returns probability distributions for each example in the batch. Note that you'll need to pay attention to the shapes when doing this. If you have a tensor a with shape (64, 10) and a tensor b with shape (64,), doing a/b will give you an error because PyTorch will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. The way to think about this is for each of the 64 examples, you only want to divide by one value, the sum in the denominator. So you need b to have a shape of (64, 1). This way PyTorch will divide the 10 values in each row of a by the one value in each row of b. Pay attention to how you take the sum as well. You'll need to define the dim keyword in  torch.sum. Setting dim=0 takes the sum across the rows while dim=1 takes the sum across the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([[5.8464e-08, 1.3344e-06, 1.5568e-14, 3.4476e-17, 1.8948e-14, 2.5883e-18,\n",
      "         6.1130e-12, 1.0000e+00, 2.4774e-19, 9.5957e-14],\n",
      "        [7.0785e-09, 4.2664e-02, 1.6387e-09, 8.3874e-17, 9.5033e-14, 2.0671e-15,\n",
      "         5.1960e-14, 9.5734e-01, 1.4512e-19, 5.9678e-11],\n",
      "        [2.3962e-04, 1.5262e-05, 8.4164e-14, 1.0631e-08, 2.2293e-18, 1.9678e-20,\n",
      "         1.8260e-12, 9.9975e-01, 4.1632e-10, 2.7432e-10],\n",
      "        [1.9982e-11, 6.5224e-11, 8.7975e-18, 4.1693e-21, 6.4339e-20, 2.0979e-23,\n",
      "         3.0004e-15, 1.0000e+00, 2.6636e-17, 1.5011e-16],\n",
      "        [1.7179e-06, 9.3274e-02, 4.6206e-10, 2.7920e-12, 2.6993e-10, 2.2434e-14,\n",
      "         9.0193e-09, 9.0672e-01, 3.3107e-15, 9.4368e-10],\n",
      "        [7.8498e-08, 1.9463e-03, 4.7368e-12, 6.2859e-14, 7.4988e-14, 4.6328e-18,\n",
      "         2.2247e-07, 9.9805e-01, 1.8778e-14, 2.2439e-12],\n",
      "        [3.7787e-06, 1.4176e-05, 2.8847e-12, 3.0566e-16, 6.5175e-16, 3.1830e-20,\n",
      "         4.6785e-10, 9.9998e-01, 5.1911e-16, 4.6265e-13],\n",
      "        [4.5291e-03, 3.4851e-03, 2.0920e-13, 1.2560e-10, 4.6248e-13, 3.1853e-15,\n",
      "         1.6711e-09, 9.9199e-01, 1.3445e-12, 1.0222e-13],\n",
      "        [5.0228e-04, 7.5813e-05, 2.0810e-09, 1.2957e-14, 1.6337e-11, 3.6008e-15,\n",
      "         3.1222e-08, 9.9942e-01, 4.2460e-15, 5.2939e-14],\n",
      "        [6.6145e-07, 5.9911e-03, 8.9271e-09, 9.0655e-13, 1.0329e-11, 7.0953e-16,\n",
      "         1.2974e-04, 9.9388e-01, 1.0307e-11, 9.5300e-07],\n",
      "        [2.4468e-04, 9.1622e-09, 3.4824e-14, 5.0096e-19, 1.9817e-13, 3.4082e-19,\n",
      "         1.2658e-12, 9.9976e-01, 2.3537e-18, 1.6108e-14],\n",
      "        [7.4452e-09, 4.0308e-05, 1.8862e-15, 2.9908e-19, 1.2856e-15, 8.8988e-17,\n",
      "         4.7526e-10, 9.9996e-01, 6.1565e-18, 8.4812e-14],\n",
      "        [1.3592e-09, 1.5823e-03, 2.0078e-10, 7.1501e-21, 6.1486e-12, 1.8329e-15,\n",
      "         5.7893e-10, 9.9842e-01, 1.6686e-14, 1.0090e-12],\n",
      "        [1.3474e-03, 4.4847e-05, 5.3951e-09, 2.5452e-13, 1.8431e-11, 7.0985e-21,\n",
      "         4.8814e-08, 9.9860e-01, 4.4112e-07, 6.3050e-06],\n",
      "        [8.1822e-06, 2.2864e-08, 3.5871e-14, 2.9037e-20, 4.5034e-17, 9.7839e-16,\n",
      "         9.3238e-11, 9.9999e-01, 7.8788e-19, 6.2624e-16],\n",
      "        [3.7963e-05, 7.3281e-01, 2.8490e-07, 2.3646e-16, 9.5404e-11, 1.1397e-17,\n",
      "         6.5041e-11, 2.6715e-01, 2.1712e-19, 2.7541e-09],\n",
      "        [3.4859e-05, 6.3882e-09, 8.7677e-11, 2.5782e-16, 3.2000e-14, 1.9380e-18,\n",
      "         9.2748e-09, 9.9997e-01, 4.9023e-14, 1.9326e-10],\n",
      "        [9.1724e-07, 5.6585e-09, 6.0212e-13, 2.5895e-14, 1.7335e-18, 1.5708e-20,\n",
      "         4.0485e-12, 1.0000e+00, 3.7428e-13, 1.2836e-14],\n",
      "        [2.4953e-09, 2.5643e-07, 2.5113e-15, 1.1290e-15, 3.2823e-16, 9.1065e-18,\n",
      "         3.9755e-12, 1.0000e+00, 1.7729e-16, 5.2262e-15],\n",
      "        [8.2580e-09, 5.5601e-06, 9.1697e-13, 6.4419e-19, 1.5953e-15, 2.3370e-17,\n",
      "         4.9109e-09, 9.9999e-01, 1.6170e-16, 2.8675e-09],\n",
      "        [1.3412e-10, 1.8944e-07, 2.1191e-17, 2.7613e-20, 4.7946e-17, 7.7192e-22,\n",
      "         8.6602e-13, 1.0000e+00, 1.0289e-16, 9.4187e-18],\n",
      "        [4.5314e-07, 9.9998e-01, 6.7743e-11, 1.0914e-19, 2.2977e-12, 2.4806e-20,\n",
      "         4.9320e-08, 2.0159e-05, 6.4414e-17, 1.2610e-08],\n",
      "        [4.1982e-06, 9.2272e-01, 1.5167e-05, 3.8312e-12, 3.7245e-08, 4.0374e-14,\n",
      "         1.3164e-08, 7.7264e-02, 1.7147e-11, 1.5236e-13],\n",
      "        [5.6310e-07, 9.6474e-08, 7.5466e-14, 8.2687e-16, 1.1833e-12, 6.7629e-18,\n",
      "         4.2244e-10, 1.0000e+00, 8.8737e-14, 3.6071e-15],\n",
      "        [3.4084e-08, 6.8601e-02, 5.7470e-12, 2.6511e-15, 9.1357e-13, 2.0548e-16,\n",
      "         1.2745e-04, 9.3127e-01, 1.6342e-11, 4.0456e-09],\n",
      "        [1.8903e-06, 3.8904e-09, 4.8976e-12, 4.0552e-21, 3.2213e-18, 4.5143e-17,\n",
      "         8.6883e-12, 1.0000e+00, 1.5073e-15, 5.2248e-10],\n",
      "        [6.4245e-06, 8.1714e-01, 5.9279e-11, 2.5308e-18, 3.6436e-12, 3.0683e-16,\n",
      "         3.9162e-11, 1.8285e-01, 1.0596e-18, 2.3383e-13],\n",
      "        [9.8377e-12, 1.6019e-05, 1.6044e-08, 9.2911e-18, 7.5184e-14, 9.3824e-16,\n",
      "         6.4401e-07, 9.9998e-01, 3.7449e-15, 5.7698e-11],\n",
      "        [3.5684e-04, 1.0710e-04, 3.5693e-10, 5.8000e-15, 3.4025e-11, 1.4644e-23,\n",
      "         2.6139e-10, 9.9954e-01, 3.0992e-15, 6.6242e-11],\n",
      "        [2.6414e-10, 2.2935e-09, 3.2416e-15, 1.9722e-19, 7.7775e-18, 5.2256e-22,\n",
      "         8.2080e-18, 1.0000e+00, 6.1479e-20, 1.3191e-19],\n",
      "        [8.7285e-09, 1.1583e-05, 3.3482e-09, 2.7344e-18, 2.9615e-14, 6.3038e-17,\n",
      "         2.1351e-11, 9.9999e-01, 1.0451e-17, 1.1733e-16],\n",
      "        [1.6704e-07, 3.5304e-07, 2.7236e-10, 1.9502e-19, 4.4293e-12, 1.1569e-18,\n",
      "         2.4063e-10, 1.0000e+00, 3.1847e-11, 3.8673e-12],\n",
      "        [1.3271e-11, 1.3358e-11, 9.0830e-17, 6.6160e-24, 1.6954e-15, 1.2603e-26,\n",
      "         8.1921e-17, 1.0000e+00, 6.6797e-20, 6.9804e-20],\n",
      "        [2.4764e-09, 1.9667e-03, 1.5015e-14, 9.8491e-23, 2.2558e-14, 5.4769e-20,\n",
      "         4.7438e-11, 9.9803e-01, 1.2064e-15, 1.3005e-15],\n",
      "        [7.2904e-10, 8.6790e-09, 3.3464e-16, 5.4471e-21, 2.3806e-20, 1.0895e-22,\n",
      "         1.3319e-12, 1.0000e+00, 1.0545e-19, 2.9997e-12],\n",
      "        [3.2087e-08, 8.2943e-05, 8.6690e-16, 2.3122e-20, 5.0276e-17, 1.4464e-16,\n",
      "         5.6615e-11, 9.9992e-01, 1.8158e-13, 2.4473e-13],\n",
      "        [1.0207e-04, 2.5758e-05, 2.7853e-12, 6.5600e-17, 1.7517e-08, 1.0807e-17,\n",
      "         6.5810e-04, 9.9921e-01, 6.9914e-18, 2.4901e-10],\n",
      "        [2.8869e-06, 1.0775e-03, 1.4590e-08, 8.6985e-18, 3.5195e-13, 2.5881e-17,\n",
      "         5.7620e-12, 9.9892e-01, 3.2899e-12, 1.7020e-11],\n",
      "        [3.2424e-13, 3.5927e-14, 2.6747e-16, 1.5450e-25, 3.8722e-16, 3.6222e-22,\n",
      "         1.8463e-15, 1.0000e+00, 1.8425e-18, 1.3167e-17],\n",
      "        [6.0267e-10, 8.5739e-04, 7.0907e-13, 2.1140e-18, 1.2507e-15, 1.5826e-20,\n",
      "         3.3519e-10, 9.9914e-01, 9.2205e-10, 1.4511e-14],\n",
      "        [4.7880e-08, 8.8850e-01, 9.0985e-09, 1.9365e-17, 1.5682e-10, 3.8515e-17,\n",
      "         3.7166e-08, 1.1150e-01, 6.5847e-15, 7.9402e-13],\n",
      "        [9.9830e-13, 2.4114e-05, 1.1436e-10, 5.6968e-20, 4.9761e-13, 1.9510e-21,\n",
      "         5.4687e-14, 9.9998e-01, 5.8449e-16, 1.0563e-14],\n",
      "        [2.5641e-11, 3.6178e-06, 5.5585e-12, 5.7182e-16, 1.9816e-17, 2.9701e-17,\n",
      "         2.2952e-09, 1.0000e+00, 2.0869e-11, 2.7155e-13],\n",
      "        [6.1788e-01, 2.2742e-04, 8.1479e-11, 1.4548e-14, 4.4162e-12, 4.7766e-19,\n",
      "         2.7525e-08, 3.8189e-01, 5.0157e-16, 3.2054e-09],\n",
      "        [4.0415e-07, 3.6452e-05, 6.1649e-14, 6.6088e-22, 4.0237e-17, 1.3794e-18,\n",
      "         1.6399e-11, 9.9996e-01, 1.7225e-18, 8.5972e-13],\n",
      "        [1.3636e-10, 8.4925e-11, 3.5813e-16, 6.8008e-26, 2.6782e-18, 1.7485e-23,\n",
      "         8.8529e-13, 1.0000e+00, 2.5970e-20, 9.4967e-19],\n",
      "        [1.7020e-04, 5.9436e-01, 3.5947e-09, 2.7668e-15, 1.4745e-08, 3.4231e-16,\n",
      "         6.6008e-08, 4.0547e-01, 7.0661e-19, 6.4840e-12],\n",
      "        [2.7715e-04, 4.9792e-06, 2.7916e-12, 3.0748e-13, 4.5120e-13, 5.0378e-18,\n",
      "         1.6360e-08, 9.9972e-01, 7.4692e-20, 1.0840e-13],\n",
      "        [1.6727e-08, 5.7929e-09, 6.8585e-14, 3.4671e-22, 3.7295e-09, 4.3390e-17,\n",
      "         8.2711e-13, 1.0000e+00, 7.9809e-12, 1.2866e-16],\n",
      "        [2.5833e-07, 9.9730e-01, 4.7117e-13, 7.2823e-18, 4.6662e-16, 2.8073e-20,\n",
      "         5.3970e-10, 2.6987e-03, 2.2708e-15, 2.8239e-13],\n",
      "        [1.1467e-09, 5.5111e-08, 1.5317e-16, 6.6946e-16, 4.1979e-16, 1.4409e-20,\n",
      "         8.5542e-12, 1.0000e+00, 8.5353e-15, 3.5785e-15],\n",
      "        [4.7415e-08, 6.5862e-01, 9.0703e-11, 2.3901e-15, 2.4722e-09, 3.2066e-13,\n",
      "         2.4401e-09, 3.4138e-01, 1.8225e-14, 9.5942e-09],\n",
      "        [4.8145e-08, 3.0017e-02, 1.8926e-08, 1.1028e-13, 9.8418e-11, 1.6292e-13,\n",
      "         2.3847e-07, 9.6998e-01, 1.5936e-08, 2.9670e-13],\n",
      "        [1.0815e-06, 1.1064e-06, 6.2319e-12, 2.1299e-17, 7.3939e-14, 1.6162e-14,\n",
      "         2.3935e-09, 1.0000e+00, 4.1986e-14, 6.6470e-11],\n",
      "        [5.4660e-04, 2.2338e-07, 1.3675e-13, 2.3113e-17, 2.2217e-13, 4.0829e-15,\n",
      "         1.6170e-10, 9.9945e-01, 9.0811e-12, 6.2356e-12],\n",
      "        [8.0201e-05, 5.5213e-04, 2.9313e-10, 1.9980e-17, 4.8069e-10, 2.1771e-14,\n",
      "         6.3624e-07, 9.9937e-01, 2.5647e-14, 2.2970e-08],\n",
      "        [3.2605e-05, 6.1920e-04, 2.9443e-10, 3.6434e-13, 1.2018e-09, 1.8027e-15,\n",
      "         2.5827e-08, 9.9935e-01, 7.2674e-16, 8.6904e-10],\n",
      "        [1.2010e-12, 1.0768e-06, 7.6028e-14, 9.4200e-18, 3.8266e-17, 1.8385e-18,\n",
      "         1.1121e-11, 1.0000e+00, 5.6366e-16, 4.0323e-14],\n",
      "        [1.9266e-04, 9.9288e-06, 4.2327e-12, 2.8448e-20, 6.2291e-16, 1.8731e-17,\n",
      "         5.2954e-09, 9.9980e-01, 4.3219e-10, 2.5914e-10],\n",
      "        [1.2508e-05, 9.8536e-01, 5.0035e-07, 8.4096e-13, 1.1443e-10, 2.4144e-14,\n",
      "         1.0875e-05, 1.4600e-02, 1.5057e-10, 1.0915e-05],\n",
      "        [8.2502e-13, 8.0549e-09, 1.5107e-20, 4.6397e-19, 3.5058e-18, 2.3624e-21,\n",
      "         7.8900e-10, 1.0000e+00, 6.3840e-17, 8.5530e-15],\n",
      "        [1.4127e-07, 2.6820e-05, 1.7345e-11, 1.8560e-22, 1.6264e-15, 1.2536e-17,\n",
      "         1.3766e-11, 9.9997e-01, 3.9862e-15, 1.3658e-14],\n",
      "        [1.9099e-07, 1.8069e-09, 1.3908e-16, 2.1005e-20, 6.0895e-20, 5.3978e-20,\n",
      "         1.4925e-14, 1.0000e+00, 4.1889e-17, 3.7539e-16],\n",
      "        [1.2296e-08, 1.6838e-08, 1.5117e-16, 5.9937e-22, 6.3035e-16, 1.3601e-24,\n",
      "         3.3722e-12, 1.0000e+00, 1.3153e-19, 8.2436e-16]])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "# .sum() makes sum of a tensor alsong row if dim=0 and column if dim=1\n",
    "# torch.sum(torch.exp(x), dim=1) has 64 values and not reshaping will divide each value in torch.exp(x) by all 64 values\n",
    "# so torch.sum(torch.exp(x), dim=1) is reshaped .view(-1,1) so every row as one element \n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)\n",
    "\n",
    "probabilities = softmax(output)\n",
    "# The sum is 1 in each rom \n",
    "print(probabilities.shape)\n",
    "print(probabilities)\n",
    "print(probabilities.sum(dim=1))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
