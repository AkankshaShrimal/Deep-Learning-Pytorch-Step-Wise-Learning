{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4-Part-1-Preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq-a13uEg1Mt"
      },
      "source": [
        "# Drive  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOD2U9cjIvzH"
      },
      "source": [
        "# https://drive.google.com/file/d/19MIw3ZI-Z91NLTDMc8GI4OlIOwN_Vv-t/view?usp=sharing\n",
        "# gdown https://drive.google.com/uc?id=19MIw3ZI-Z91NLTDMc8GI4OlIOwN_Vv-t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4sAZu-Ng5Mn",
        "outputId": "c34decb2-82a1-4a9b-e625-018781099f94"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je2BFNT-opjK"
      },
      "source": [
        "# Import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYyMYGBJg3d7"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure \n",
        "import json\n",
        "import datetime\n",
        "import copy\n",
        "from PIL import Image\n",
        "from PIL import Image as im\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import math as Math\n",
        "import random\n",
        "import torch.optim"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ddCgsLg3-r"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWrdLQrlg4FM"
      },
      "source": [
        "import cv2\n",
        "import joblib "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJHXf9lSMLAT"
      },
      "source": [
        "# !pip install scipy==1.1.0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-AosU5yMFic"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from scipy.misc import imread, imresize\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhHZBqE-RDsc",
        "outputId": "dc4f150a-d2c8-4d7b-b12a-6f9a2807e14e"
      },
      "source": [
        " #Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import string\n",
        "import nltk\n",
        "import joblib\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# #Reading Files\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbk5lxy-g4bg"
      },
      "source": [
        "# Saving and Loading functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K5lvjV5g6hD"
      },
      "source": [
        "# Saving and Loading models using joblib \n",
        "def save(filename, obj):\n",
        "  with open(filename, 'wb') as handle:\n",
        "      joblib.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load(filename):\n",
        "  with open(filename, 'rb') as handle:\n",
        "      return joblib.load(filename)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gngDNkEfckBp"
      },
      "source": [
        "# Paths  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwYcMdBdcmJa"
      },
      "source": [
        "# Final folder where all the data saved \n",
        "# output_folder = \"/content/drive/MyDrive/SEM-2/05-DL /Assignments/A4/Method2/data/\"\n",
        "output_folder = \"./\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l46rC17g7X_"
      },
      "source": [
        "# Data Downloading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2b-4LmCg9FV"
      },
      "source": [
        "# # !wget https://drive.google.com/file/d/1lbqTV-u8xmZ3eBuQ4tUSjq0mOjaIIp_P \n",
        "# !unzip \"/content/drive/MyDrive/SEM-2/05-DL /Assignments/A4/Data/Dataset.zip\" -d /content/drive/MyDrive/SEM-2/05-DL /Assignments/A4/Data/"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsfNmXoTxA8z"
      },
      "source": [
        "# !du -sh \"/content/drive/MyDrive/A4/Data/\"\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhOZHNV2hZvF"
      },
      "source": [
        "\n",
        "# !unzip \"/content/drive/MyDrive/SEM-2/05-DL /Assignments/A4/Data/Dataset.zip\" -d \"/content/drive/MyDrive/SEM-2/05-DL /Assignments/A4/Data/myData/\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vAHuHtN_coM"
      },
      "source": [
        "# Data Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxwIp-3bpXe-"
      },
      "source": [
        "## Loading Data Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpCOdqSspU9y"
      },
      "source": [
        "# IITD\n",
        "# p = \"/content/drive/MyDrive/SEM-2/05-DL /Assignments/A4/\"\n",
        "# gmail \n",
        "p = \"/content/drive/MyDrive/A4/\"\n",
        "# !ls \"/content/drive/MyDrive/SEM-2/05-DL /Assignments/A4/Data/myData/Data/Val\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tUqCGz5_Bsv"
      },
      "source": [
        "f1 = load(p + \"Data/myData/Data/Val/val_captions.pkl\")\n",
        "f2 = load(p + \"Data/myData/Data/Test/test_captions.pkl\")\n",
        "f3 = load(p + \"Data/myData/Data/Train/train_captions.pkl\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rkrMcKe_twt"
      },
      "source": [
        "# \"\"\"\n",
        "#     Creates input files for training, validation, and test data.\n",
        "#     :param image_folder: folder with downloaded images\n",
        "#     :param captions_per_image: number of captions to sample per image\n",
        "#     :param min_word_freq: words occuring less frequently than this threshold are binned as <unk>s\n",
        "#     :param output_folder: folder to save files\n",
        "#     :param max_len: don't sample captions longer than this length\n",
        "#     \"\"\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iagu82lvRxXg"
      },
      "source": [
        "## word frequency counter "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGK2hxXxTFUZ"
      },
      "source": [
        "def inputPreprocess(documentText):\n",
        "  def remove_punc(word):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punc_list = [char if char not in punctuations else ' ' for char in word]\n",
        "    return ''.join(no_punc_list)\n",
        "\n",
        "  #Convert the text to lower case\n",
        "  documentText = documentText.lower()\n",
        "  \n",
        "  #removing punctuations from the string\n",
        "  no_punc_sentence = remove_punc(documentText)\n",
        "  #documentText = documentText.translate(str.maketrans('','',string.punctuation))\n",
        "\n",
        "  #Perform word tokenization\n",
        "  listToken = word_tokenize(no_punc_sentence)\n",
        "\n",
        "  # #removing stopwords from tokens\n",
        "  # stopWords = set(stopwords.words('english'))  \n",
        "  # filtered_sentence = [token for token in listToken if token not in stopWords]   \n",
        "\n",
        "\n",
        "  # #remove blank space tokens\n",
        "  # no_space_sentence = [token for token in filtered_sentence if token.strip()]   \n",
        "  \n",
        "  # #removing single char word and \\x type chars\n",
        "  # #filtered_sentence = [token for token in listToken if len(token) > 1]      \n",
        "  \n",
        "  return listToken"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PES72zaXSAGa"
      },
      "source": [
        "# calculates the total word frequency for all train,test and val \n",
        "word_freq = Counter()\n",
        "for file in (f1,f2,f3):\n",
        "  for k in file.keys():\n",
        "    for sentence in file[k]:\n",
        "      word_freq.update(inputPreprocess(sentence))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBIrO3PmMAnf"
      },
      "source": [
        "# word_freq"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtoRbe0EULO9"
      },
      "source": [
        "## Creating a word map "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6oUS1SrRIjT"
      },
      "source": [
        "# mapping each word to a particular index \n",
        "min_word_freq = 1\n",
        "# Create word map\n",
        "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "word_map = {k: v + 4 for v, k in enumerate(words)}\n",
        "word_map['<start>'] = 0\n",
        "word_map['<eos>'] = 1\n",
        "word_map['<unk>'] = 2\n",
        "word_map['<pad>'] = 3\n",
        "\n",
        "word_map_reverse = {str(v + 4) : k for v, k in enumerate(words)}\n",
        "word_map_reverse['0'] = '<start>'\n",
        "word_map_reverse['1'] = '<eos>'\n",
        "word_map_reverse['2'] = '<unk>'\n",
        "word_map_reverse['3'] = '<pad>'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzFZBzcUVG51"
      },
      "source": [
        "# word_map_reverse"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9lRlOm3VJsE"
      },
      "source": [
        "## Train, Test, Val paths and captions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GnHL_ScUfF4"
      },
      "source": [
        "# Read image paths and captions for each image\n",
        "train_image_paths = []\n",
        "train_image_captions = []\n",
        "val_image_paths = []\n",
        "val_image_captions = []\n",
        "test_image_paths = []\n",
        "test_image_captions = []"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arlQe-aJVnRG"
      },
      "source": [
        "# Not giving absolute path but variable path or relative path here \n",
        "image_folder = \"Data\"\n",
        "base_folder = [\"Val\", \"Test\",\"Train\"] "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzbFv8A-Ugw_"
      },
      "source": [
        "# With maximum length 50 we get 5 captions for each image in Train,Test,Val \n",
        "max_len = 50\n",
        "for idx,file in enumerate((f1,f2,f3)):\n",
        "  for k in file.keys():\n",
        "    path = os.path.join(image_folder,base_folder[idx],'Images/'+ k)\n",
        "    \n",
        "    for sentence in file[k]:\n",
        "      if len(inputPreprocess(sentence)) <= max_len:\n",
        "        captions = inputPreprocess(sentence)\n",
        "\n",
        "      # if len(captions) == 0:\n",
        "      #       continue\n",
        "\n",
        "        if idx == 2:\n",
        "          train_image_paths.append(path)\n",
        "          train_image_captions.append(captions)\n",
        "        elif idx == 0:\n",
        "          val_image_paths.append(path)\n",
        "          val_image_captions.append(captions)\n",
        "        elif idx == 1:\n",
        "          test_image_paths.append(path)\n",
        "          test_image_captions.append(captions)        \n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAJEjMnrZ8lB"
      },
      "source": [
        "# Got paths of all images and their array of captions for each Train, Test and Val \n",
        "# check if lengths equal or not \n",
        " # Sanity check\n",
        "assert len(train_image_paths) == len(train_image_captions)\n",
        "assert len(val_image_paths) == len(val_image_captions)\n",
        "assert len(test_image_paths) == len(test_image_captions)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnDz-KK1jXMF"
      },
      "source": [
        "# remove half of the images and captions in training \n",
        "train_image_paths = train_image_paths[:10000]\n",
        "train_image_captions = train_image_captions[:10000]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYaPp-aaWUvR",
        "outputId": "07d4313d-a6d4-429b-e855-1d190c1867aa"
      },
      "source": [
        "len(train_image_paths)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9D9I6s2beDw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13ef7323-2c69-4763-e7ff-82bd13d37420"
      },
      "source": [
        "train_image_paths[0]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Data/Train/Images/2513260012_03d33305cf.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hddNrxAscYGQ"
      },
      "source": [
        "## Saving word map "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuuZe77xcMdD"
      },
      "source": [
        "# # # Save word map to a JSON\n",
        "\n",
        "# with open(output_folder + '/word_dict.json', 'w') as f:\n",
        "#         json.dump(word_map, f)\n",
        "\n",
        "save('./word_dict.json',word_map)\n",
        "save('./word_dict_reverse.json',word_map_reverse)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkEEE2zNc_Pe"
      },
      "source": [
        "## Saving Image, Captions, Caption lengths "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGo5_ZI9c6JF"
      },
      "source": [
        "# Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files\n",
        "seed(123)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOfjixQ0pisQ"
      },
      "source": [
        "def process_caption_tokens(caption_tokens, word_dict, max_length):\n",
        "    captions = []\n",
        "    for tokens in caption_tokens:\n",
        "        token_idxs = [word_dict[token] if token in word_dict else word_dict['<unk>'] for token in tokens]\n",
        "        captions.append(\n",
        "            [word_dict['<start>']] + token_idxs + [word_dict['<eos>']] +\n",
        "            [word_dict['<pad>']] * (max_length - len(tokens)))\n",
        "\n",
        "    return captions"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBtXrn_9pv5F"
      },
      "source": [
        "max_length = 50\n",
        "train_image_captions = process_caption_tokens(train_image_captions, word_map, max_length)\n",
        "val_image_captions = process_caption_tokens(val_image_captions, word_map, max_length)\n",
        "test_image_captions = process_caption_tokens(test_image_captions, word_map, max_length)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U96gy6qvqHiL"
      },
      "source": [
        "  # with open(output_folder + '/train_img_paths.json', 'w') as f:\n",
        "  #     json.dump(train_image_paths, f)\n",
        "  # with open(output_folder + '/val_img_paths.json', 'w') as f:\n",
        "  #     json.dump(val_image_paths, f)\n",
        "  # with open(output_folder + '/test_img_paths.json', 'w') as f:\n",
        "  #    json.dump(test_image_paths, f)\n",
        "  # with open(output_folder + '/train_captions.json', 'w') as f:\n",
        "  #     json.dump(train_image_captions, f)\n",
        "  # with open(output_folder + '/val_captions.json', 'w') as f:\n",
        "  #     json.dump(val_image_captions, f)\n",
        "  # with open(output_folder + '/test_captions.json', 'w') as f:\n",
        "  #     json.dump(test_image_captions, f)\n",
        "\n",
        "save('./train_img_paths.json',train_image_paths)\n",
        "save('./val_img_paths.json',val_image_paths)\n",
        "save('./test_img_paths.json',test_image_paths)\n",
        "save('./train_captions.json',train_image_captions)\n",
        "save('./val_captions.json',val_image_captions)\n",
        "save('./test_captions.json',test_image_captions)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LjTV3jfDJyss",
        "outputId": "83b1fd97-f3ed-4707-a79f-024f0a9d97e9"
      },
      "source": [
        "# downloading all preprocessed files from collab \n",
        "from google.colab import files\n",
        "files.download('train_img_paths.json')\n",
        "files.download('val_img_paths.json')\n",
        "files.download('test_img_paths.json')\n",
        "files.download('train_captions.json')\n",
        "files.download('val_captions.json')\n",
        "files.download('test_captions.json')\n",
        "files.download('word_dict.json')\n",
        "files.download('word_dict_reverse.json')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_90970fce-59f9-44f5-afa9-7d2ce72116d1\", \"word_dict_reverse.json\", 82254)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2K-2BwEfxDt"
      },
      "source": [
        "# Preprocessed Files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoBWl7wOf0aD",
        "outputId": "7007389d-f95d-41ae-e51e-deb39c1e6cbb"
      },
      "source": [
        "# !ls \"/content/drive/MyDrive/SEM-2/05-DL /Assignments/A4/Method2/data/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_captions.json   train_captions.json   val_captions.json   word_dict.json\n",
            "test_img_paths.json  train_img_paths.json  val_img_paths.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlAXJaJgsUEZ",
        "outputId": "f836cc7e-d3be-4d83-ee23-e2e287658f86"
      },
      "source": [
        "# !ls "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t    test_img_paths.json   val_captions.json\n",
            "sample_data\t    train_captions.json   val_img_paths.json\n",
            "test_captions.json  train_img_paths.json  word_dict.json\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}