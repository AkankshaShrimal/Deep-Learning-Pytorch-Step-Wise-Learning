{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Neural Network\n",
    "\n",
    "#### h1 and h2 are two units in hidden layer\n",
    "\n",
    "The first layer shown on the bottom here are the inputs, understandably called the input layer. The middle layer is called the hidden layer, and the final layer (on the right) is the output layer. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} &amp; w_{12} \\\\\n",
    "           w_{21} &amp;w_{22} \\\\\n",
    "           \\vdots &amp;\\vdots \\\\\n",
    "           w_{n1} &amp;w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$\n",
    "\n",
    "The number of hidden units a parameter of the network, often called a hyperparameter to differentiate it from the weights and biases parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyTorch\n",
    "import torch\n",
    "def activation(x):\n",
    "    return 1/(1+ torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate some data\n",
    "torch.manual_seed(7) #set the random seeds so things are predictible\n",
    "#Creating feature vector\n",
    "features = torch.randn((1,3))\n",
    "\n",
    "#Define size of each layer in the network\n",
    "n_input = features.shape[1]   # number of input units must match \n",
    "n_hidden = 2                  # number of hidden units\n",
    "n_output = 1                  # number of output units\n",
    "\n",
    "#Weights from input to hidden layer\n",
    "w1 = torch.randn(n_input, n_hidden)\n",
    "#Weights from hidden to output layer\n",
    "w2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "#Bias terms for hidden and output layer\n",
    "B1 = torch.randn((1,n_hidden))\n",
    "B2 = torch.randn((1,n_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "hidden_output = activation(torch.mm(features,w1) + B1)\n",
    "output = activation(torch.mm(hidden_output ,w2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use torch.from_numpy(). To convert a tensor to a Numpy array, use the .numpy() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7276185  0.7920595  0.6081663 ]\n",
      " [0.82972493 0.12551915 0.33593043]\n",
      " [0.35597365 0.18755575 0.36365027]\n",
      " [0.28455485 0.33333747 0.66910846]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7276, 0.7921, 0.6082],\n",
      "        [0.8297, 0.1255, 0.3359],\n",
      "        [0.3560, 0.1876, 0.3637],\n",
      "        [0.2846, 0.3333, 0.6691]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#Convert numpy array to tensor\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7276185 , 0.7920595 , 0.6081663 ],\n",
       "       [0.82972493, 0.12551915, 0.33593043],\n",
       "       [0.35597365, 0.18755575, 0.36365027],\n",
       "       [0.28455485, 0.33333747, 0.66910846]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.910474   3.168238   2.43266519]\n",
      " [3.31889972 0.50207659 1.34372172]\n",
      " [1.42389461 0.75022299 1.45460107]\n",
      " [1.13821941 1.3333499  2.67643385]]\n",
      "tensor([[2.9105, 3.1682, 2.4327],\n",
      "        [3.3189, 0.5021, 1.3437],\n",
      "        [1.4239, 0.7502, 1.4546],\n",
      "        [1.1382, 1.3333, 2.6764]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#Changing tensor changes numpy array also\n",
    "b.mul_(2)\n",
    "print(a)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
