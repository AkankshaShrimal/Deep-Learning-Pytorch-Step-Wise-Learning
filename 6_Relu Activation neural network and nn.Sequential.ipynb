{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building Neural network\n",
    "<img src='https://user-images.githubusercontent.com/24764528/50548729-e5597f00-0c77-11e9-9229-0a6f795249a7.png' width='400px'></p>\n",
    " Create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the nn.ReLU module or F.relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering data \n",
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets , transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "#Batch size determines we are getting 64 images at once \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Inputs in actual images for digit classification\n",
    "inputs = images.view(64,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making model - this model gives random probabilities and has not been trained yet.\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(784,128)\n",
    "        self.hidden2 = nn.Linear(128,64)\n",
    "        self.output = nn.Linear(64,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.hidden1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.softmax(x,dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], grad_fn=<SumBackward2>)\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "result = model.forward(inputs)\n",
    "\n",
    "print(result.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "The weights are automatically initialized, but it's possible to customize\n",
    "\n",
    "**model.fc1.weight** return weights attached to layer for current instance.\n",
    "\n",
    "\n",
    "For custom initialization, we want to modify these tensors in place. These are actually autograd Variables, \n",
    "so we need to get back the actual tensors with model.fc1.weight.data. Once we have the tensors,\n",
    "we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0198,  0.0355,  0.0347,  ..., -0.0292,  0.0302,  0.0333],\n",
      "        [-0.0309, -0.0161,  0.0329,  ..., -0.0146, -0.0318,  0.0028],\n",
      "        [ 0.0078, -0.0287,  0.0054,  ...,  0.0319,  0.0334, -0.0291],\n",
      "        ...,\n",
      "        [ 0.0040, -0.0120, -0.0149,  ...,  0.0101, -0.0041, -0.0310],\n",
      "        [-0.0142, -0.0214,  0.0074,  ..., -0.0027, -0.0321,  0.0113],\n",
      "        [-0.0226,  0.0048,  0.0278,  ..., -0.0070, -0.0098,  0.0173]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0223, -0.0199, -0.0334,  0.0075, -0.0151,  0.0207,  0.0039,  0.0329,\n",
      "         0.0029, -0.0196, -0.0329,  0.0270,  0.0298, -0.0263, -0.0185,  0.0251,\n",
      "         0.0333, -0.0271,  0.0345, -0.0269,  0.0078,  0.0297, -0.0063, -0.0143,\n",
      "        -0.0007, -0.0281, -0.0212, -0.0277, -0.0225, -0.0177,  0.0203,  0.0292,\n",
      "         0.0311,  0.0350,  0.0111, -0.0282, -0.0100, -0.0085, -0.0303, -0.0126,\n",
      "        -0.0219,  0.0259, -0.0155, -0.0074,  0.0132,  0.0278,  0.0252, -0.0182,\n",
      "        -0.0039,  0.0034, -0.0330,  0.0014, -0.0065,  0.0124, -0.0011,  0.0272,\n",
      "         0.0052,  0.0260, -0.0221, -0.0090, -0.0229,  0.0020,  0.0243, -0.0088,\n",
      "         0.0015,  0.0317, -0.0319, -0.0240,  0.0357, -0.0132,  0.0077,  0.0171,\n",
      "        -0.0334, -0.0306, -0.0125,  0.0266,  0.0081,  0.0068,  0.0197,  0.0097,\n",
      "         0.0045,  0.0058,  0.0264,  0.0032,  0.0145, -0.0076,  0.0247, -0.0215,\n",
      "         0.0085,  0.0011,  0.0011,  0.0037, -0.0006, -0.0011,  0.0268,  0.0313,\n",
      "         0.0314,  0.0024,  0.0128,  0.0011,  0.0068,  0.0241, -0.0296,  0.0345,\n",
      "         0.0289, -0.0017,  0.0116, -0.0045,  0.0220, -0.0024, -0.0195,  0.0159,\n",
      "         0.0314, -0.0142, -0.0350, -0.0055,  0.0194,  0.0089, -0.0080,  0.0249,\n",
      "         0.0009,  0.0036,  0.0334,  0.0079, -0.0255,  0.0132, -0.0049,  0.0353],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Change them by following way \\n         \\n    # Set biases to all zeros\\n    model.fc1.bias.data.fill_(0)\\n\\n    # sample from random normal with standard dev = 0.01# sampl \\n    model.fc1.weight.data.normal_(std=0.01)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return weights and bias for current layer\n",
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)\n",
    "'''\n",
    "    Change them by following way \n",
    "         \n",
    "    # Set biases to all zeros\n",
    "    model.fc1.bias.data.fill_(0)\n",
    "\n",
    "    # sample from random normal with standard dev = 0.01# sampl \n",
    "    model.fc1.weight.data.normal_(std=0.01)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Sequential\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, nn.Sequential.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters for our network# Hyper \n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OrderedDict\n",
    "You can also pass in an OrderedDict to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so each operation must have a different name.\n",
    "\n",
    "\n",
    "Now you can access layers either by integer or the name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from  collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=784, out_features=128, bias=True)\n",
      "Linear(in_features=784, out_features=128, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model[0])\n",
    "print(model.fc1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
