{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "PyTorch provides a module nn that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units, 10 output units and a softmax output.\n",
    "\n",
    "\n",
    "**class Network(nn.Module):**\n",
    "\n",
    "Here we're inheriting from nn.Module. Combined with super().__init__() this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from nn.Module when you're creating a class for your network. \n",
    "\n",
    "**self.hidden = nn.Linear(784, 256)**\n",
    "\n",
    "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to self.hidden. The module automatically creates the weight and bias tensors which we'll use in the forward method. You can access the weight and bias tensors once the network (net) is created with net.hidden.weight and net.hidden.bias.\n",
    "\n",
    "\n",
    "**self.softmax = nn.Softmax(dim=1)**\n",
    "\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting dim=1 in nn.Softmax(dim=1) calculates softmax across the columns.\n",
    "\n",
    "def forward(self, x):\n",
    "PyTorch networks created with nn.Module must have a forward method defined. It takes in a tensor x and passes it through the operations you defined in the __init__ method.\n",
    "\n",
    "\n",
    "Now we can create a Network object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn   \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn Module is used to create large networks\n",
    "# sub-class Network from nn.Module\n",
    "\n",
    "class Network(nn.Module):\n",
    "    # super calls _init_ of nn.module to register all functionality added \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        nn.Linear - creates an operation for linear trandformations \n",
    "        has its own parameters defined for all the layer transformations.\n",
    "        nn.linear(input nodes, output nodes)\n",
    "        '''\n",
    "        \n",
    "        #Inputs to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        #Output layer 10 units one for each\n",
    "        self.output = nn.Linear(256,10)\n",
    "        '''\n",
    "        Functional definations can also be used for softmax and sigmoid - no need to create additonal functions in models\n",
    "    \n",
    "        '''\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Pass input tensor to each ofthese operatons \n",
    "        x = self.hidden(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn.functional module** - This is the most common way you'll see networks defined as many operations are simple element-wise functions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
